\chapter{Introduction}

Amidst the explosion of data globalization, content streaming, and an increased usage of cloud applications, a demand for faster and more reliable data transferring has arisen. An initial go to has been to use TCP for providing reliable communication and making adjustments to TCP to increase speed such as \cite{brakmo1995tcp}\cite{wei2006fast}\cite{ha2008cubic} as well as others mentioned in \cite{ha2008cubic}\cite{He2002}. The TCP protocol has been the foundation for numerous systems and protocols as noted here \cite{Fan2010} as well as FTP \cite{Bhushan1972}. This abstraction comes with an inheritance of drawbacks found in TCP; there is a performance hit for connections that have large Round-Trip-Times (RTT) and also under utilizes bandwidth \cite{Fan2010}, especially with network links like \cite{Pfister2001} and other Gigabit links. This under utilization stems from the insurance of reliability that TCP provides.

Reliability is an important feature for many applications that use computer communication. Some solutions, \cite{Allman1995}\cite{Allman1997}\cite{Sivakumar2000psockets}, seek to address the problem using a parallel approach. This approach can increase bandwidth use, but it adds overhead on the performance of the machines at the ends of the network connection. With this approach, the application layer needs to divide the data evenly across all parallel connections, then the TCP connections further breakdown the data into packets before sending. The reverse process happens when reconstructing the data on the receiver end. This two stage partitioning and reconstructing of data adds overhead to the application layer and can limit just how much of a speedup is achieved with parallel connections. The other shortcoming of parallel connections is the limit to the parallelism of a machine. If a computer has a two core processor, it can only do two jobs in parallel. Additionally, parallelism could result in wasted resources in the instance a core finishes all of its work before other cores. Parallelism may have improvements to network utilization, but that is met with costs on the computer \cite{Leijen2009}.

Along with the use of parallel computing, there have been adaptions of the higher-level protocols and systems mentioned in \cite{Fan2010} that utilize UDP as the transport for packets from sender to receiver \cite{He2002}\cite{Aspera2016}\cite{Fan2010}\cite{Meiss2007}\cite{gu2007udt}. This option is vary viable method for mitigating the problem of under utilized bandwidth and long connections. There is a challenge, however, to using UDP. As per the original RFC, UDP is an unreliable transport protocol; however, UDP is incredibly fast with throughput of data. This places work on the application layer to provide reliability for data transfers. With the physical layer increasing in bandwidth constantly, the bottleneck lies with the end points for a connection \cite{Aspera2016}\cite{Fan2010}. Solutions using UDP need to mitigate as much of the end point bottleneck as possible to provide consistently fast data transfers.

This report outlines the design of a lean protocol, MCDTP, for maximizing bandwidth usage by using asynchronous technology to add efficient concurrency, the challenges faced with this project, and lessons learned. The motivation for this project has been to gain a deeper understanding of Computer Networking and possibly shed light on benefits and expenses of networking using the approach of this project. The source code for this project will be made available online and released under the MIT license as an open source project. The goal of this project is to contribute to the advancement of Computer Networking.
