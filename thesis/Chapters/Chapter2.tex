\chapter{Background}

In order to approach the problem in a unique way, it was necessary to have an understanding of the networking protocols, existing technologies and solutions by other researchers. This background is presented in this section.

\section{Networking}

The Transmission Control Protocol (TCP) was invented in 1978 to provide a ``reliable host-to-host'' protocol for network communication \cite{cerf1978specification}. By using packet reception acknowledgements and congestion control, TCP provided reliable communication atop the Internet Protocol and became the standard for network communication in 1983 \cite{andrews2013who}. The User Datagram Protocol (UDP) was created in 1980 as an alternative to TCP for applications that needed faster communication \cite{postel1980user,kozierokr2005udp}.

The TCP transport protocol utilizes two major mechanisms to provide reliable communication: 1) a sliding window coupled with a packet acknowledgement system, and 2) a congestion control system to minimize the occurrence of a lost packet \cite{cerf1978specification}. The sliding window mechanism works by having a view of $x$ packets of a larger packet buffer that are sent in series. The sender will transmit $x$ packets to the receiver and wait for acknowledgements (ACK) by the receiver \cite{cerf1978specification}. When the first packet in the window is acknowledged, the sender slides the window so that packets waiting to be transmitted enter the view of the window. A packet that is unacknowledged is retransmitted until acknowledged.

Retransmission events are the result of a lost transmitted packet, or a lost ACK packet \cite{cerf1978specification}. These events trigger a congestion control algorithm to take over to try and mitigate the occurrence of another lost packet \cite{allman2009tcp}. The algorithms part of the initial specification for TCP were slow start, congestion avoidance, fast retransmit, and fast recovery. Slow start and congestion avoidance are used by the sender to minimize the number of ``outstanding data being injected into the network'' \cite{allman2009tcp}. The receiver plays a role with the initiation of fast retransmit and fast recovery by notifying the sender that a segment of data has arrived out of order.

In 1977, a committe called Open Systems Interconnection (OSI) designed an architecture for computer communication, referred to as network stack OSI-model \cite{zimmermann1980osi}. This network stack provided a standard method for networking. The OSI-model consisted of 7 layers (bottom to top): Physical, Data Link, Network, Transport, Session, Presentation, and Application \cite{zimmermann1980osi}. The OSI-model was later replaced by a 4 layer model (TCP/IP-model) that would become the requirement in order for computers to utilize the the Internet Protocol \cite{braden1989requirements}. The TCP/IP-model has the following layers (bottom to top): Link, Network, Transport, and Application.

\subsection{Optimizing TCP}

The congestion control system TCP-Vegas \cite{brakmo1995tcp} uses the Round-Trip-Time (RTT) to measure throughput and a lower bound threshold and upper bound threshold to perform congestion control. If the lower bound is greater than the measured throughput, TCP-Vegas will increase the sending rate of the packets. If the measured throughput exceeds the upper bound threshold, it throttles how fast the packets are being sent. Consequently, this method reduces the number of retransmits and the sliding window of TCP can continually move forward \cite{brakmo1995tcp}.

Wei et al. designed FAST to uses RTT and packet loss as measures for congestion control \cite{wei2006fast}. This method tries to maximize bandwidth usage by being agressive with increasing window sizes until the RTT gets close to a threshold. FAST takes an optimistic approach when exceeding the threshold. It will slowly decrease the window size in hopes that the RTT had degraded for a brief moment. Only when it worsens does FAST aggressively decrease the window size. As a result, all paths in a connection equally share the effects of a bottleneck and thus mitigating the ebbs and flows of traffic providing a consistent throughput \cite{wei2006fast}.

The Binary Increase Congestion Control (BIC-TCP) \cite{xu2004binary} uses two mechanisms: binary search increase and additive increase. The binary search increase component uses the logarithmic nature of a binary search to rapidly increase the window size at the beginning of transmission and gradually decreases the rate of growth. The additive increase is used as a method of shifting the starting point of the binary search increase in fixed steps. This component tries to prevent the logarithmic steps of the binary search increase from being too aggressive to the network. The BIC-TCP congestion control system was able to achieve a 95\% network utilization \cite{xu2004binary}.

The CUBIC \cite{ha2008cubic} is the successor to the BIC-TCP congestion control system. Rather than using a logarithmic function like BIC-TCP, CUBIC uses a cubic function to increase the window size. This provides a much steadier growth in window size inducing less stress on the network. The concave profile of the cubic function is how CUBIC starts transmission as well as recovers from a packet loss event. The convex profile is the behavior CUBIC takes once it has recovered and has exceeded the window size at the moment of packet loss. The CUBIC system achieves the same network utilization as BIC-TCP, however, the more granular control over window size imposes less stress on the network allowing for friendlier sharing of network resources with other machines \cite{ha2008cubic}.

\subsection{Application Layer Protocols}

The Transport layer protocols like TCP and UDP are the foundation for higher level protocols that attempt to optimize throughput at the Application layer \cite{He2002,Fan2010,Allman1995,Allman1997,Sivakumar2000psockets,Aspera2016,Meiss2007,gu2007udt,lai2009designing}.

\subsubsection{UDP Based Protocols}

Tsunami \cite{Meiss2007} uses both UDP and TCP to transmit data. Tsunami uses UDP for sending the payload and TCP as a feedback loop. The TCP connection provides signaling for packet loss, retransmission requests, error reporting, and completion report. For retransmission, the sender will interrupt the flow of the data transfer to resend the data block in which the packet loss occurred. This approach achieved a 400-450 Mbps throughput when transferring a 5 GB file \cite{Meiss2007}.

Similar to Tsunami in its mechanics, RBUDP \cite{He2002} uses a TCP connection to send messages between sender and receiver. The major difference is the retransmission mechanism. Retransmission for RBUDP uses the UDP connection after the ``bulk data transmission phase'' has finished, which is repeated until all packets have made it to the receiver. RBUDP was able to upper bounded packet loss to 7\% and achieve a throughput of 550Mbps.

The UDP Data Transfer (UDT) protocol \cite{gu2007udt} is strictly a UDP-based protocol that imposes a congestion control mechanism on top of that. There are two UDP sockets used by UDT, one socket labeled ``Sender'', the other labeled ``Receiver'' \cite{gu2007udt}. In addition to receiving data, the receiver socket is used to send control messages back to the sender. UDT used TCP like messages for congestion control and reliability insurance. During a transfer from Chicago to Amsterdam, UDT had a throughput of 940Mbps over a 1Gbps link.

Aspera's fast and secure protocol (fasp) \cite{Fan2010,Aspera2016} is a UDP-based protocol that uses a rate-based control system by using the RTT of a packet, referred to as ``packet delay'', to adjust the rate in which packets are being transmitted. The fasp protocol handles retransmissions by resending packets at a rate that makes use of available bandwidth. This provides a continuous flow of data. Aspera is capable of utilizing 95\% of the available network bandwidth \cite{Fan2010}.

\subsection{Network Stack Optimization}

The Advanced Data Transfer Service (ADTS) \cite{lai2009designing} is a replacement layer for the Network and Transport layer and is intended to be used with InfiniBand \cite{Pfister2001} (InfiniBand is a network stack that treats network traffic as communication rather than bussed data). The ADTS uses a zero-copy communication system to reduce host latency. It mitigates the need for the CPU to copy data to different places in memory according to the application's needs. Instead, a send operation will send data directly to an address in memory on the receiver. This mitigation of unnecessary CPU usage and allowance of direct memory access provided a speed up of 65\% compared to using UDP.

\subsection{Optimization Through Concurrency}

The multi-socket FTP (XFTP) \cite{Allman1995,Allman1997} is a modified version of FTP \cite{postel1980user} that uses multiple TCP connections to transfer a single file. The file is divided into records, with the condition that the number of records is greater than the number of connections. When a connection has the resources to send, it is assigned a record to transmit. The receiver reconstructs the original file by placing records in their correct positions. The XFTP could achieve a 90\% network utilization using eight connections \cite{Allman1995}.

Sivakumar et al. designed PSockets \cite{Sivakumar2000psockets} to be a socket programming library that enables the use of parallel TCP sockets. The library provides simple send and receive functions that abstract the mechanics of PSockets. The passed in data is segmented and sent across multiple TCP connections. The receiver reassembles the received data back into its original form. PSockets manages multiple sockets asynchronously so that sockets do not need to simultaneously share resources. PSockets achieved a throughput of 70 Mbps.

\section{Asynchrony}

Asynchrony is the method by which processes can communicate without waiting for responses \cite{He1990}. Processes that use asynchrony for communication are referred to as asynchronous processes \cite{He1990,Josephs1989}. These asynchronous processes use unbounded buffered channels for input and output messages \cite{He1990,Josephs1989}. Consider the scenario where there are three processes, $P_1$, $P_2$, and $P_3$. Process $P_3$ submits a message to the input channel of $P_1$ and then $P_2$ submits a message to the input channel of $P_3$. If these processes where synchronous processes, $P_3$ would be forced to wait for $P_1$ to submit a response over the output channel before it can process the message from $P_2$ \cite{He1990}. With asynchronous processes, $P_3$ can process the message from $P_2$ while it waits for a response from $P_1$ \cite{He1990}. The incorporation of asynchrony as a modern programming technique is referred to as ``asynchronous programming'' \cite{syme2011f}.

\subsection{.NET Framework Asynchrony Model}

The .NET Framework by Microsoft provides implementations of asynchrony in the C\# and F\# languages \cite{Leijen2009,syme2011f}. The Task Parallel Library (TPL) is the foundation for asynchrony in the .NET Framework \cite{Leijen2009,msftVFS}. The TPL library uses application threads to represent asynchronous processes \cite{Leijen2009}. The messages that are communicated between processes are computation work tasks and their results \cite{Leijen2009}. For instance, if a computation task requires querying a database, it may choose to perform that asynchronously. In this scenario, the query task is submitted to another process to perform. The original asynchronous process performing the computation task can move on to the next work task.

The TPL library was first introduced in 2009 and in later updates to C\#, the keywords ``async'' and ``await'' simplified asynchronous programming \cite{alexdavies2012}. The F\# language has an ``async'' keyword as well to simplify asynchronous programming \cite{syme2011f}. These keywords provide an abstraction to using the TPL library by invoking a routine in the respective compiler that modifies user written code to leverage TPL \cite{alexdavies2012, syme2011f}. Consider an asynchronous program that has a computation component, a database query component, and another computation component. Using the keywords for the respective language, the respective compiler will transform the outlined components into separate work tasks \cite{alexdavies2012,syme2011f}. As each task is completed, the subsequent task, referred to as continuations, is submitted to another asynchronous process \cite{syme2011f}.
