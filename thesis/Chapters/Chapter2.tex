\chapter{Background}

In order to approach the problem in a unique way, it was necessary to have an understanding of the networking protocols, existing technologies and solutions by other researchers. This background is presented in this section.

\section{Networking}

The Transmission Control Protocol (TCP) was invented in 1978 to provide a ``reliable host-to-host" protocol for network communication \cite{cerf1978specification}. By using packet reception acknowledgements and congestion control, TCP provided reliable communication atop the Internet Protocol and became the standard for network communication in 1983 \cite{andrews2013who}. The User Datagram Protocol (UDP) was created in 1980 as an alternative to TCP \cite{postel1980user}\cite{kozierokr2005udp}. The purpose for designing UDP was for applications that needed faster communication.

The TCP transport protocol utilizes two major mechanisms to provide reliable communication: 1) a sliding window coupled with a packet acknowledgement system, and 2) a congestion control system to minimize the occurrence of a lost packet \cite{cerf1978specification}. The sliding window mechanism works by having a view of $x$ packets, referred to as window where $x$ is the window size, of a larger packet buffer that are sent in series. The sender will transmit $x$ packets to the receiver and wait for acknowledgements (ACK) by the receiver \cite{cerf1978specification}. When the first packet in the window is acknowledged, the sender slides the window so that packets waiting to be transmitted enter the view of the window. A packet that is unacknowledged is retransmitted until acknowledged.

Retransmission events are the result of a lost transmitted packet, or a lost ACK packet \cite{cerf1978specification}. These events trigger a congestion control algorithm to take over to try and mitigate the occurrence of another lost packet \cite{allman2009tcp}. The algorithms part of the initial specification for TCP were slow start, congestion avoidance, fast retransmit, and fast recovery. Slow start and congestion avoidance are used by the sender to minimize how the number of "outstanding data being injected into the network" \cite{allman2009tcp}. The receiver plays a role with the initiation of fast retransmit and fast recovery by notifying the sender that a segment of data has arrived out of order.

In 1977, a committe called Open Systems Interconnection (OSI) designed an architecture for computer communication, referred to as network stack OSI-model \cite{zimmermann1980osi}. This network stack provided a standard method for networking. The OSI-model consisted of 7 layers (bottom to top): Physical, Data Link, Network, Transport, Session, Presentation, and Application \cite{zimmermann1980osi}. The OSI-model was later replaced by a 4 layer model (TCP/IP-model) that would become the requirement in order for computers to utilize the the Internet protocol \cite{braden1989requirements}. The TCP/IP-model has the following layers (bottom to top): Link, Network, Transport, and Application.

\subsection{Optimizing TCP}

The congestion control system TCP-Vegas \cite{brakmo1995tcp} uses RTT to measure throughput and a lower bound threshold and upper bound threshold to perform congestion control. If the lower bound is greater than the measured throughput, TCP-Vegas will increase the sending rate of the packets. If the measured throughput exceeds the upper bound threshold, it throttles how fast the packets are being sent. Consequently, this method reduces the number of retransmits and the sliding window of TCP can continually move forward \cite{brakmo1995tcp}.

Wei, David X and Jin, Cheng and Low, Steven H and Hegde, Sanjay designed FAST to uses RTT and packet loss as measures for congestion control. This method tries to maximize bandwidth usage by being agressive with increasing window sizes until the RTT gets close to a threshold. FAST takes an optimistic approach when exceeding the threshold. It will slowly decrease the window size in hopes that the RTT had degraded for a brief moment. Only when it worsens does FAST aggressively decrease window size. As a result, all paths in a connection equally share the effects of a bottleneck and thus mitigating the ebbs and flows of traffic providing a consistent throughput \cite{wei2006fast}.

The Binary Increase Congestion Control (BIC-TCP) \cite{xu2004binary} uses two mechanisms: binary search increase and additive increase. The binary search increase component uses the logarithmic nature of a binary search to rapidly increase the window size at the beginning of transmission and gradually decreases the rate of growth. The additive increase is used as a method of shifting the starting point of the binary search increase in fixed steps so that the logarithmic steps of the binary search increase at the beginning are not too large. This component tries to prevent the binary search increase from being too aggressive to the network. The BIC-TCP congestion control system was able to achieve a 95\% network utilization \cite{xu2004binary}.

The CUBIC \cite{ha2008cubic} is the successor to the BIC-TCP congestion control system. Rather than using a logarithmic function like BIC-TCP, CUBIC uses a cubic function to increase the window size. This provides a much steadier growth in window size inducing less stress on the network. The concave profile of the cubic function is how CUBIC starts transmission as well as recovers from a packet loss event. The convex profile is the behavior CUBIC takes once it has recovered and has exceeded the window size at the moment of packet loss. The CUBIC system achieves the same network utilization as BIC-TCP, however, the more granular control over window size imposes less stress on the network allowing for friendlier sharing of network resources with other machines \cite{ha2008cubic}.

\subsection{Application Layer Protocols}

The Transport layer protocols like TCP and UDP are the foundation for higher level protocols that attempt to optimize throughput at the Application layer \cite{He2002}\cite{Fan2010}\cite{Allman1995}\cite{Allman1997}\cite{Sivakumar2000psockets}\cite{Aspera2016} \cite{Meiss2007}\cite{gu2007udt}\cite{lai2009designing}.

\subsubsection{UDP Based Protocols}

Tsunami \cite{Meiss2007} uses both UDP and TCP to transmit data. Tsunami uses UDP for sending the payload and TCP as a feedback loop. The TCP connection provides signaling for packet loss, retransmission requests, error reporting, and completion report. For retransmission, the server will interrupt the flow to resend the data block in which the packet loss occurred. This approach achieved a 400-450 Mbps throughput when transferring a 5 GB file \cite{Meiss2007}.

Similar to Tsunami in its mechanics, RBUDP \cite{He2002} uses a TCP connection to send messages between sender and receiver. The major difference is the retransmission mechanism. Retransmission for RBUDP uses the UDP connection after the ``bulk data transmission phase" has finished, which is repeated until all packets have made it to the receiver. RBUDP was able to upper bounded packet loss to 7\% and achieve a throughput of 550Mbps.

The UDP Data Transfer protocol \cite{gu2007udt} is strictly a UDP-based protocol that imposes a congestion control mechanism on top of that. There are two UDP sockets used by UDT, one socket labeled ``Sender", the other labeled ``Receiver". The receiver socket would not only receive data, but it would be used for sending control information. UDT used TCP like messages for congestion control and reliability insurance. During a transfer from Chicago to Amsterdam, UDT had a throughput of 940Mbps over a 1Gbps link.

Aspera's fasp protocol \cite{Fan2010}\cite{Aspera2016} is a UDP-based protocol that uses a rate-based control system by using the RTT of a packet, referred to as ``packet delay", to adjust the rate in which packets are being transmitted. fasp handles retransmissions by resending packets at a rate that makes use of available bandwidth. This provides a continuous flow of data. Aspera is capable of transferring 1TB of data over the course of 2.2 hours \cite{Fan2010}.

\subsection{Network Stack Optimization}

The Advanced Data Transfer Service (ADTS) \cite{lai2009designing} is a replacement layer for the Network and Transport layer and is intended to be used with InfiniBand \cite{Pfister2001} (InfiniBand is a network stack that treats network traffic as communication rather than bussed data). ADTS uses a zero-copy communication system to reduce host latency. It mitigates the need for the CPU to copy data to different places in memory according to the application's needs. Instead, a send operation will send data directly to an address in memory on the receiver. This mitigation of unnecessary CPU usage and allowance of direct memory access provided a speed up of 65\% compared to using UDP.

\subsection{Optimization Through Concurrency}

The multi-socket FTP (XFTP) \cite{Allman1995}\cite{Allman1997} is a modified version of FTP \cite{postel1980user} that uses multiple TCP connections to transfer a single file. The file is divided into records, with the condition that the number of records is greater than the number of connections. When a connection has the resources to send, it is assigned a record to transmit. The receiver reconstructs the original file by placing records in their correct positions. The XFTP could achieve a 90\% network utilization using eight connections.

Sivakumar et al. designed PSockets \cite{Sivakumar2000psockets} to be a socket programming library that enables the use of parallel TCP sockets. The library provides simple send and receive functions that abstract the mechanics of PSockets. The passed in data is segmented and sent across multiple TCP connections. The receiver reassembles the received data back into its original form. PSockets manages multiple sockets asynchronously so that sockets do not need to simultaneously share resources. PSockets achieved a throughput of 70 Mb/s.

\section{Asynchrony}

Asynchrony started out as a way for processes to communicate with each other using unbounded input and output buffer channels \cite{Josephs1989}\cite{He1990}. In a scenario where an asynchronous process $P_1$ has a queue full of input messages, if $P_1$ needs to send a message to another asynchronous process $P_2$, $P_1$ does not need to suspend itself to wait for a response message from $P_2$ \cite{He1990}. This concept has worked its way into programming models and work scheduling \cite{Guo2009}\cite{Leijen2009}\cite{syme2011f}.

\subsection{Scheduling Policies}

There are two major policies for scheduling asynchronous work, work-first and help-first \cite{Guo2009}. The work-first policy, also known as work-stealing, schedules spawned tasks for immediate execution and any continuation of that task is placed in a queue where other workers can steal from. This policy is particularly useful in situations where workers are busy and rarely need to steal from other workers. The help-first policy takes the opposite approach. A worker will ask help from other workers to begin executing spawned tasks. The continuations of the task are executed by the original worker. This policy is useful when stealing is very frequent because stealing can be implemented in parallel and thus increases the throughput of scheduled tasks.

\subsection{.NET Framework Asynchrony Model}

The .NET Framework by Microsoft provides implementations of asynchrony in their languages C\# and F\# \cite{Leijen2009}\cite{syme2011f}. Both languages use the TPL library to incorporate asynchrony technology \cite{Leijen2009}\cite{msftVFS}. The TPL library was first introduced in 2009 and in later updates to C\#, the keywords ``async" and ``await" simplified asynchronous programming \cite{alexdavies2012}. In C\#, these keywords instruct the compiler to transform user written code into tasks that operate asynchronously that are represented as $Task$ objects \cite{Leijen2009}. The F\# language uses computation expressions, a feature exclusive to F\#, to build asynchronous work flows represented as $Async$ objects \cite{syme2011f}. It is worth noting that the underlying implementation of F\#'s $Async$ object uses C\#'s $Task$ object, which means that any difference in performance is marginal.

The TPL library represents an asynchronous process as a thread \cite{Leijen2009}. The thread receives $Task$ objects as input messages on the input buffer channel. The snippet of code that the $Task$ object encapsulates is executed on the thread and the return value is sent over the output channel to the parent process or $Task$ object \cite{Leijen2009}. The TPL library uses a pool of threads, referred to as a $ThreadPool$, that collectively handle $Task$ objects. The $ThreadPool$ uses a work-stealing policy to schedule $Task$ objects for execution to maximize its thoughput.
